{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiation in Autograd\n",
    "\n",
    "The autograd – an auto differentiation module in PyTorch – is used to calculate the derivatives and optimize the parameters in neural networks. It is intended primarily for gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s use a simple tensor and set the requires_grad parameter to true. This allows us to perform automatic differentiation and lets PyTorch evaluate the derivatives using the given value which, in this case, is 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use a simple equation y = 3x<sup>2</sup> as an example and take the derivative with respect to variable x. So, let’s create another tensor according to the given equation. Also, we’ll apply a neat method .backward on the variable y that forms acyclic graph storing the computation history, and evaluate the result with .grad for the given value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 3*x**2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of the equation at x=3 is: tensor(18.)\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(\"Derivative of the equation at x=3 is:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "\n",
    "PyTorch generates derivatives by building a backwards graph behind the scenes, while tensors and backwards functions are the graph’s nodes. In a graph, PyTorch computes the derivative of a tensor depending on whether it is a leaf or not.\n",
    "\n",
    "PyTorch will not evaluate a tensor’s derivative if its leaf attribute is set to True. We won’t go into much detail about how the backwards graph is created and utilized, because the goal here is to give you a high-level knowledge of how PyTorch makes use of the graph to calculate derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data attribute of the tensor: tensor(3.)\n",
      "grad attribute of the tensor:: tensor(18.)\n",
      "grad_fn attribute of the tensor:: None\n",
      "is_leaf attribute of the tensor:: True\n",
      "requires_grad attribute of the tensor:: True\n"
     ]
    }
   ],
   "source": [
    "print('data attribute of the tensor:',x.data)\n",
    "print('grad attribute of the tensor::',x.grad)\n",
    "print('grad_fn attribute of the tensor::',x.grad_fn)\n",
    "print(\"is_leaf attribute of the tensor::\",x.is_leaf)\n",
    "print(\"requires_grad attribute of the tensor::\",x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data attribute of the tensor: tensor(27.)\n",
      "grad attribute of the tensor: None\n",
      "grad_fn attribute of the tensor: <MulBackward0 object at 0x0000025F84AF8DF0>\n",
      "is_leaf attribute of the tensor: False\n",
      "requires_grad attribute of the tensor: True\n"
     ]
    }
   ],
   "source": [
    "print('data attribute of the tensor:',y.data)\n",
    "print('grad attribute of the tensor:',y.retain_grad())\n",
    "print('grad_fn attribute of the tensor:',y.grad_fn)\n",
    "print(\"is_leaf attribute of the tensor:\",y.is_leaf)\n",
    "print(\"requires_grad attribute of the tensor:\",y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a more complicated equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the equation is: tensor(64., grad_fn=<AddBackward0>)\n",
      "Derivative of the equation at x = 3 is: tensor(38.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = 6*x**2 + 2*x + 4\n",
    "print(\"Result of the equation is:\", y)\n",
    "y.backward()\n",
    "print(\"Derivative of the equation at x = 3 is:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Partial Derivatives of Functions\n",
    "\n",
    "PyTorch also allows us to calculate partial derivatives of functions. For example, if we have to apply partial derivation to the following function:\n",
    "\n",
    " - f(u, v) = u<sup>3</sup> + v<sup>2</sup> + 4uv\n",
    "\n",
    " Now, let’s do it the PyTorch way, where u = 3 and v = 4.\n",
    "\n",
    "We’ll create u, v and f tensors and apply the .backward attribute on f in order to compute the derivative. Finally, we’ll evaluate the derivative using the .grad with respect to the values of u and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(4., requires_grad=True)\n",
      "tensor(91., grad_fn=<AddBackward0>)\n",
      "Partial Derivative w.r.t. u: tensor(43.)\n",
      "Partial Derivative w.r.t. v: tensor(20.)\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor(3.0, requires_grad=True)\n",
    "v = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "f = u**3 + v**2 + 4*u*v\n",
    "\n",
    "print(u)\n",
    "print(v)\n",
    "print(f)\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(\"Partial Derivative w.r.t. u:\", u.grad)\n",
    "print(\"Partial Derivative w.r.t. v:\", v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Successive Differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nth_derivative(f, wrt, n=2):\n",
    "    for i in range(n):\n",
    "        grads = grad(f, wrt, create_graph=True)[0]\n",
    "        f = grads.sum()\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x**2 + x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double derivative\n",
    "nth_derivative(f,x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
